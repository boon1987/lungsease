project: "Lung Disease Project"
workspace: khanghua.boon
description: Lung_PyTorch_const
entrypoint: det_model_def:LungDiseaseTrial
resources:
    devices: []
    max_slots: null                 # max slots per experiment.
    #native_parallel: false
    resource_pool: gpu-v100         # specifying the resource pool.
    shm_size: null                  # size of /dev/shm.
    slots_per_trial: 4              
    weight: 1                       # used in fair-share scheduler
    priority: null                  # used in priority scheduler.
max_restarts: 1                     # Maximum number of restarting the trial if the trial fails. 

# Custom 0: Validation Policy
#records_per_epoch: 60000
perform_initial_validation: true     # Whether to perform validation action before the training start.
min_validation_period:               # Specifying the temporal interval for validation action, i.e, perform validation after specified number of batches, records or epochs from previous validation action.
    batches: 100       

# Custom 1: bind mount for nfs stored data
bind_mounts: 
  - container_path: /run/determined/workdir/shared_fs
    host_path: /mnt/mapr_nfs/determined 
    propagation: rprivate 
    read_only: false 

# Custom 2: Custom docker images
environment:
  image: 
    cuda: boon1987/mlde_lung_disease:latest
# With environment + resource_manager + resource_pools in the configuration yaml file, we can specify the following scheduling behaviour:
#   Determined Native: Fairshare, Priority
#   K8s special: Gang, Priority

# Custom 3: Profiling
profiling:
  enabled: true
  begin_on_batch: 0
  end_after_batch: 99
  sync_timings: true

# Custom 4: Reproducibility
reproducibility:
  experiment_seed: 123

# Custom 5: Checkpointing
checkpoint_policy: best                               # the value are either: best, all or none
checkpoint_storage:
  type: shared_fs
  host_path: /mnt/mapr_nfs/determined/det_checkpoints
  storage_path: null                                  # must be subdirectory of host_path. If unset, the checkpoints are read and written to host_path
  propagation: rprivate
  save_experiment_best: 0                             # During the traiing, save such number of top-k best checkpoints that are collected ans sorted over all trials. 
  save_trial_best: 5                                 # During the training, each trial in an experiment save such number of best checkpoints according to the metric in searcher section. 
  save_trial_latest: 3                                # After experiment ends, save such number of checkpoints for each trial. Note that if the latest checkpoint is one of the best checkpoints in save_trial_best, then it does not save any extra older checkpoint.
min_checkpoint_period:                                # Specify the temporal interval for checkpointing action, i.e, create a new checkpoint after specified number of batches, records or epochs from previous checkpointing action.
  #records:
  #epochs:
  batches: 0

# Custom 6: Hyper-parameter searches
hyperparameters:
    dataloader_num_worker: 2
    dataloader_prefetch_factor: 20
    seed: 123
    amp_mode: False
    global_batch_size: 192
    lr: 0.001                     # using smaller learning rate is better
    weight_decay: 1e-5
    data_root: /run/determined/workdir/shared_fs/LungDiseaseDataset/CheXpert-v1.0-small
searcher:
    name: single
    metric: val_auc_mean
    smaller_is_better: false
    max_length:
        epochs: 3
        #batches: 100


# Custom 7: Model Registry
# Custom 8: Trial vs Core API differences
# Custom 9: Model HUB
# Custom 10: HPC Workload manager (not k8s)
    # Slurm
    # PBS



# hyperparameters:
#   global_batch_size: 32
#   init_features: 32
#   input_channels: 3
#   output_channels: 1
#   learning_rate:
#     type: log
#     minval: -5
#     maxval: -1
#     base: 10
#   weight_decay:
#     type: log
#     minval: -8
#     maxval: -3
#     base: 10
#   pretrained: True # starting from pretrained unet model from PyTorch Hub - not necessary if loading weights from older trial
#   split_seed: 1
#   validation_ratio: 0.2

#   # data augmentation hyperparameters
#   hflip_augmentation:
#     type: categorical
#     vals:
#       - True
#       - False
#   affine_augmentation:
#     type: categorical
#     vals:
#       - True
#       - False
#   max_rotation: # images can be rotated by up to this number of degrees, in either direction
#     type: int
#     minval: 10
#     maxval: 180
#   max_translation:
#     type: double
#     minval: 0.05
#     maxval: 0.3
#   min_scale:
#     type: double
#     minval: 0.5
#     maxval: 0.9
#   max_scale:
#     type: double
#     minval: 1.1
#     maxval: 1.5
# searcher:
#   name: adaptive_asha
#   metric: val_dice
#   smaller_is_better: False
#   max_length:
#     epochs: 25
#   max_trials: 8
#   max_concurrent_trials: 2

